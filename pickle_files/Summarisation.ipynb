{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "progressive-compression",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9237ba05-2011-4a43-9f2d-e114139f9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,TfidfTransformer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from scipy.spatial.distance import cdist\n",
    "import sumy\n",
    "from flair.embeddings import FlairEmbeddings, TransformerWordEmbeddings , TransformerDocumentEmbeddings, SentenceTransformerDocumentEmbeddings\n",
    "from flair.data import Sentence\n",
    "import spacy\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "### Word2Vec Embedding Initialisation\n",
    "\n",
    "# get word2vec pretrained model \n",
    "model_w2v = KeyedVectors.load_word2vec_format('/Users/William/Desktop/Thesis_code/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "### Glove Embedding  Initialisation\n",
    "\n",
    "model_glove =pd.read_pickle('/Users/William/Desktop/Thesis_code/glove_dict.pickle')\n",
    "\n",
    "\n",
    "### BERT Sentence Embedding  Initialisation\n",
    "\n",
    "bert_emb = SentenceTransformerDocumentEmbeddings('roberta-base')\n",
    "\n",
    "# read in the data \n",
    "Hillary = pd.read_pickle('StanceDataset/Hillary.pkl')\n",
    "Hillary_favor = pd.read_pickle('StanceDataset/Hillary_favor.pkl')\n",
    "Hillary_against = pd.read_pickle('StanceDataset/Hillary_against.pkl')\n",
    "Hillary_neither = pd.read_pickle('StanceDataset/Hillary_neither.pkl')\n",
    "Abortion = pd.read_pickle('StanceDataset/Abortion.pkl')\n",
    "Abortion_favor = pd.read_pickle('StanceDataset/Abortion_favor.pkl')\n",
    "Abortion_against = pd.read_pickle('StanceDataset/Abortion_against.pkl')\n",
    "Abortion_neither = pd.read_pickle('StanceDataset/Abortion_neither.pkl')\n",
    "Atheism = pd.read_pickle('StanceDataset/Atheism.pkl')\n",
    "Atheism_favor = pd.read_pickle('StanceDataset/Atheism_favor.pkl')\n",
    "Atheism_against = pd.read_pickle('StanceDataset/Atheism_against.pkl')\n",
    "Atheism_neither = pd.read_pickle('StanceDataset/Atheism_neither.pkl')\n",
    "Climatechange = pd.read_pickle('StanceDataset/Climatechange.pkl')\n",
    "Climatechange_favor = pd.read_pickle('StanceDataset/Climatechange_favor.pkl')\n",
    "Climatechange_against = pd.read_pickle('StanceDataset/Climatechange_against.pkl')\n",
    "Climatechange_neither = pd.read_pickle('StanceDataset/Climatechange_neither.pkl')\n",
    "Feministmovement = pd.read_pickle('StanceDataset/Feministmovement.pkl')\n",
    "Feministmovement_favor = pd.read_pickle('StanceDataset/Feministmovement_favor.pkl')\n",
    "Feministmovement_against = pd.read_pickle('StanceDataset/Feministmovement_against.pkl')\n",
    "Feministmovement_neither = pd.read_pickle('StanceDataset/Feministmovement_neither.pkl')\n",
    "\n",
    "\n",
    "gsHillary_favor = pd.read_pickle('gsHillary_favor.pkl')\n",
    "gsHillary_against = pd.read_pickle('gsHillary_against.pkl')\n",
    "gsClimatechange_favor = pd.read_pickle('gsClimatechange_favor.pkl')\n",
    "gsAbortion_favor = pd.read_pickle('gsAbortion_favor.pkl')\n",
    "gsAbortion_against = pd.read_pickle('gsAbortion_against.pkl')\n",
    "gsAtheism_favor = pd.read_pickle('gsAtheism_favor.pkl')\n",
    "gsAtheism_against = pd.read_pickle('gsAtheism_against.pkl')\n",
    "gsFeministmovement_favor = pd.read_pickle('gsFeministmovement_favor.pkl')\n",
    "gsFeministmovement_against = pd.read_pickle('gsFeministmovement_against.pkl')\n",
    "\n",
    "\n",
    "\n",
    "model_hil_favor = pd.read_pickle('StanceDataset/model_favor_hil.pkl')\n",
    "model_hil_against =pd.read_pickle('StanceDataset/model_against_hil.pkl')\n",
    "model_cc_favor =  pd.read_pickle('StanceDataset/model_favor_cc.pkl')\n",
    "model_cc_against = pd.read_pickle('StanceDataset/model_against_cc.pkl') \n",
    "model_ab_favor =  pd.read_pickle('StanceDataset/model_favor_ab.pkl')\n",
    "model_ab_against =pd.read_pickle('StanceDataset/model_against_ab.pkl')\n",
    "model_ath_favor =pd.read_pickle('StanceDataset/model_favor_ath.pkl')\n",
    "model_ath_against =pd.read_pickle('StanceDataset/model_against_ath.pkl')\n",
    "model_fm_favor =pd.read_pickle('StanceDataset/model_favor_fm.pkl')\n",
    "model_fm_against =pd.read_pickle('StanceDataset/model_against_fm.pkl')\n",
    "\n",
    "\n",
    "H_fv = Hillary_favor.loc[Hillary_favor['Opinion Towards']== 'Argumentative']\n",
    "H_ag = Hillary_against.loc[Hillary_against['Opinion Towards'] == 'Argumentative']\n",
    "Ab_fv = Abortion_favor.loc[Abortion_favor['Opinion Towards'] == 'Argumentative']\n",
    "Ab_ag = Abortion_against.loc[Abortion_against['Opinion Towards'] == 'Argumentative']\n",
    "Ath_fv = Atheism_favor.loc[Atheism_favor['Opinion Towards'] == 'Argumentative']\n",
    "Ath_ag = Atheism_against.loc[Atheism_against['Opinion Towards'] == 'Argumentative']\n",
    "Fm_fv = Feministmovement_favor.loc[Feministmovement_favor['Opinion Towards'] == 'Argumentative']\n",
    "Fm_ag = Feministmovement_against.loc[Feministmovement_against['Opinion Towards'] == 'Argumentative']\n",
    "Cl_fv = Climatechange_favor.loc[Climatechange_favor['Opinion Towards'] == 'Argumentative']\n",
    "Cl_ag = Climatechange_against.loc[Climatechange_against['Opinion Towards'] == 'Argumentative']\n",
    "\n",
    "### Word2vec combined with tfidf feature scaling \n",
    "\n",
    "# function to get tfidf * word emb vector \n",
    "def tfidf_scaled_wv(data,model,dim):\n",
    "    text = []\n",
    "    for i in data['tokenize']:\n",
    "        string = ' '.join(i)\n",
    "        text.append(string)\n",
    "    tfidf = TfidfVectorizer(stop_words =None)\n",
    "    tf_idf_tr = tfidf.fit_transform(text)\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    sentences = data['tokenize'].values\n",
    "    data_dtm = pd.DataFrame(tf_idf_tr.toarray(), columns = feature_names)\n",
    "    tfidf_sent_vectors = [] \n",
    "    row = 0\n",
    "    for sentence in sentences:\n",
    "        sent_vec = np.zeros(dim)\n",
    "        weight_sum = 0\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vec = model[word]\n",
    "                tfidf = data_dtm.iloc[row][word]\n",
    "                sent_vec += (vec * tfidf)\n",
    "                weight_sum += tfidf\n",
    "            except:\n",
    "                pass\n",
    "            # check if weight_sum is zero meaning all words in that tweet gave no context \n",
    "        if weight_sum != 0:     \n",
    "            sent_vec  /= weight_sum     \n",
    "        tfidf_sent_vectors.append(sent_vec)\n",
    "        row+=1\n",
    "    return tfidf_sent_vectors\n",
    "  \n",
    "\n",
    "word_emb = tfidf_scaled_wv(Hillary_favor,model_w2v,300)\n",
    "word_emb = np.array(word_emb)\n",
    "\n",
    "### Sentence Embedding Using BERT\n",
    "\n",
    "\n",
    "def get_embeddings(text):\n",
    "    sentence = Sentence(text)\n",
    "    bert_emb.embed(sentence)\n",
    "    return sentence.embedding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sentence_vec(data):\n",
    "    # the returned embedding is a tensor so we used the pytorch method to detach into a numpy array\n",
    "    detach = lambda x : x.detach().numpy()\n",
    "    emb  = lambda x : get_embeddings(x)\n",
    "    data['sen_emb'] = data.Processed_Tweet.apply(emb)\n",
    "    data['sen_emb'] = data.sen_emb.apply(detach)\n",
    "#     data['sen_emb'] = data.sen_emb.apply(lambda x: np.array(x))\n",
    "\n",
    "### Visualize KMedoids to decide on optimal cluster \n",
    "\n",
    "def silhouette_method(word_emb):\n",
    "    model = KMedoids(metric='cosine')\n",
    "    # k is range of number of clusters.\n",
    "    visualizer = KElbowVisualizer(model, k=(2,15),metric='silhouette', timings= True)\n",
    "    visualizer.fit(word_emb)        # Fit the data to the visualizer\n",
    "#     visualizer.show()        # Finalize and render the figure\n",
    "    k = visualizer.elbow_value_\n",
    "    return k\n",
    "\n",
    "k =silhouette_method(word_emb)\n",
    "\n",
    "def calinski_method(word_emb):\n",
    "    model = KMedoids(metric='cosine')\n",
    "# k is range of number of clusters.\n",
    "    visualizer = KElbowVisualizer(model, k=(2,30),metric='calinski_harabasz', timings= True)\n",
    "    visualizer.fit(word_emb)        # Fit the data to the visualizer\n",
    "#     visualizer.show()        # Finalize and render the figure\n",
    "    k = visualizer.elbow_value_\n",
    "    return k\n",
    "\n",
    "k = calinski_method(word_emb)\n",
    "\n",
    "## Clustering using kmedoids  to generate summaries\n",
    "\n",
    "### Word Vector Clustering ( Word2Vec , GloVe , Fasttext)\n",
    "\n",
    "def clustering_w2v(corpus,model,dim):\n",
    "    data = corpus.copy()\n",
    "    #get the word vector \n",
    "    word_emb = np.array(tfidf_scaled_wv(data,model,dim))\n",
    "    #find the average optimal k clusters using silhouette method and calinski method\n",
    "    k_1 = silhouette_method(word_emb)\n",
    "    k_2 = calinski_method(word_emb)\n",
    "    opt_k = int((k_1 + k_2) / 2)\n",
    "    kmedoids = KMedoids(metric= 'cosine', n_clusters = opt_k)\n",
    "    kmedoids.fit(word_emb)\n",
    "    assigned_clusters = kmedoids.labels_\n",
    "    dist_clusters = kmedoids.transform(word_emb)\n",
    "    sq_dis = dist_clusters ** 2\n",
    "    cluster_centers = kmedoids.cluster_centers_\n",
    "    # distance matrix  using cosine similarity\n",
    "    dm_cos = cdist(word_emb, cluster_centers, metric='cosine')\n",
    "    dm_eucl = cdist(word_emb, cluster_centers, metric='euclidean')\n",
    "    dis_eucl = np.array([dm_eucl[i][x] for i,x in enumerate(assigned_clusters)])\n",
    "    dis_cos = np.array([dm_cos[i][x] for i,x in enumerate(assigned_clusters)])\n",
    "    data['cluster'] = assigned_clusters\n",
    "    data['centroid'] = data.cluster.apply(lambda x: cluster_centers[x])\n",
    "    data['cosine_dis_from_centroid'] = dis_cos\n",
    "    data['euclidean_dis_from_centroid'] =dis_eucl\n",
    "    return data,opt_k\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "### Sentence Vector (BERT) Clustering  \n",
    "\n",
    "def clustering_sen2v(corpus):\n",
    "    data = corpus.copy()\n",
    "    #get the word vector\n",
    "    sen_emb = data['sen_emb'].tolist()\n",
    "    sen_emb = np.array(sen_emb)\n",
    "    #find the average optimal k clusters using silhouette method and calinski method\n",
    "    k_1 = silhouette_method(sen_emb)\n",
    "    k_2 = calinski_method(sen_emb)\n",
    "    opt_k = int((k_1 + k_2) / 2)\n",
    "    kmedoids = KMedoids(metric= 'cosine', n_clusters = opt_k)\n",
    "    kmedoids.fit(sen_emb)\n",
    "    assigned_clusters = kmedoids.labels_\n",
    "    dist_clusters = kmedoids.transform(sen_emb)\n",
    "    sq_dis = dist_clusters ** 2\n",
    "    cluster_centers = kmedoids.cluster_centers_\n",
    "    # distance matrix  using cosine similarity\n",
    "    dm_cos = cdist(sen_emb, cluster_centers, metric='cosine')\n",
    "    dm_eucl = cdist(sen_emb, cluster_centers, metric='euclidean')\n",
    "    dis_eucl = np.array([dm_eucl[i][x] for i,x in enumerate(assigned_clusters)])\n",
    "    dis_cos = np.array([dm_cos[i][x] for i,x in enumerate(assigned_clusters)])\n",
    "    data['cluster'] = assigned_clusters\n",
    "    data['centroid'] = data.cluster.apply(lambda x: cluster_centers[x])\n",
    "    data['cosine_dis_from_centroid'] = dis_cos\n",
    "    data['euclidean_dis_from_centroid'] =dis_eucl\n",
    "    return data,opt_k\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# analyis of cluster to generate effective summaries by measure of distances \n",
    "def cluster_dis_summary(data,k,num_tweets=None):\n",
    "    # list of tweets around centroid and tweet closest to centroid for each cluster\n",
    "    count = 0\n",
    "    index = 0\n",
    "#   cluster number\n",
    "    cn = 1\n",
    "    tweets_cos = []\n",
    "    tweets_eucl = []\n",
    "    # k is the number of cluster\n",
    "    while count != num_tweets:\n",
    "        df = data.loc[data['cluster'] == cn]\n",
    "        # mean distance to cluster centroid         \n",
    "        a = df['cosine_dis_from_centroid'].mean()\n",
    "        b = df['euclidean_dis_from_centroid'].mean()\n",
    "        # pool of tweets based on cosine  and euclidean distance\n",
    "        summ_cos = df.loc[df['cosine_dis_from_centroid'] <= a]\n",
    "        summ_eucl = df.loc[df['euclidean_dis_from_centroid'] <= b]\n",
    "        summ_cos = summ_cos.sort_values(['cosine_dis_from_centroid'])\n",
    "        summ_eucl = summ_eucl.sort_values(['euclidean_dis_from_centroid'])\n",
    "        #tweet closest to centroid based on cosine and euclidean dsitance\n",
    "        closest_cos = summ_cos.iloc[index]\n",
    "        closest_eucl = summ_eucl.iloc[index]\n",
    "        # add them to our lists \n",
    "        tweets_cos.append(closest_cos['Tweet'])\n",
    "        tweets_eucl.append(closest_eucl['Tweet'])\n",
    "       #update cluster number      \n",
    "        cn = (cn + 1) % k \n",
    "      #update count \n",
    "        count +=1\n",
    "#     update index only if we checked all clusters\n",
    "        if  count % k == 0:\n",
    "            index +=1\n",
    "#             print('count:',count)\n",
    "#             print('index :',index)\n",
    "        \n",
    "    return tweets_cos,tweets_eucl\n",
    "    \n",
    "        \n",
    "                \n",
    "        \n",
    "    \n",
    "\n",
    "### Baseline Summarization using implementation of SumBasic  Algorithm\n",
    "\n",
    "from sumy.summarizers.sum_basic import SumBasicSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "import nltk\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatize = True\n",
    "rm_stopwords = True\n",
    "\n",
    "\n",
    "# my own implementation of sumbasic\n",
    "def clean_sentence(tokens):\n",
    "    if lemmatize: tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    if rm_stopwords: tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "def get_probabilities(data):\n",
    "    word_ps = {}    \n",
    "    token_count = 0\n",
    "    for sentence in data['Processed_Tweet_sw']:\n",
    "        sentence = nltk.word_tokenize(sentence)\n",
    "        tokens = clean_sentence(sentence)\n",
    "        token_count += len(tokens)\n",
    "        for token in tokens:\n",
    "            if token not in word_ps:\n",
    "                word_ps[token] = 1\n",
    "            else:\n",
    "                word_ps[token] += 1\n",
    "    for word_p in word_ps:\n",
    "        word_ps[word_p] = word_ps[word_p]/float(token_count)   \n",
    "    return word_ps\n",
    "\n",
    "def score_sentence(sentence,word_ps):\n",
    "    score = 0\n",
    "    num_tokens = 0\n",
    "    tokenize_sentence = nltk.word_tokenize(sentence)\n",
    "    tokens = clean_sentence(tokenize_sentence)\n",
    "    for token in tokens:\n",
    "        if  token in word_ps:\n",
    "            score += word_ps[token]\n",
    "            num_tokens +=1 \n",
    "    if tokens == []: return 0\n",
    "    return float(score)/float(num_tokens)\n",
    "\n",
    "\n",
    "def max_sentence(sentences,word_ps):\n",
    "    max_sentence =  None\n",
    "    max_score =  float('-inf')\n",
    "    for sentence in sentences['Processed_Tweet_sw']:\n",
    "        score = score_sentence(sentence,word_ps)\n",
    "        if score > max_score or max_score ==  float('-inf'):\n",
    "            max_sentence = sentence\n",
    "            max_score = score\n",
    "    update_ps(max_sentence,word_ps)\n",
    "    return max_sentence\n",
    "\n",
    "def update_ps(max_sentence, word_ps):\n",
    "    sentence = nltk.word_tokenize(max_sentence)\n",
    "    tokens = clean_sentence(sentence)\n",
    "    for word in tokens:\n",
    "        word_ps[word] = word_ps[word]**2\n",
    "\n",
    "# baseline function to find summary of tweets\n",
    "def orig_sumBasic(df,num_sentences):\n",
    "    word_ps = get_probabilities(df)\n",
    "    summary = []\n",
    "    summ = []\n",
    "    for i in range(num_sentences):\n",
    "        summary.append(max_sentence(df, word_ps))\n",
    "    for s in summary:\n",
    "        for index, row in df.iterrows():\n",
    "            if row['Processed_Tweet_sw'] == s:\n",
    "                summ.append(row['Tweet'])\n",
    "                break\n",
    "    return summ\n",
    "\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "def textrank(df,gold_standard):\n",
    "    results =[]\n",
    "    values = df.Processed_Tweet_sw.apply(lambda x: x + '.')\n",
    "    text = ' '.join(values.tolist())\n",
    "    summary = summarize(text,len(gold_standard)/len(df),split=True)\n",
    "    summary = [summ.replace(\".\", \"\") for summ in summary]\n",
    "    for s in summary:\n",
    "        for index, row in df.iterrows():\n",
    "            if row['Processed_Tweet_sw'] == s:\n",
    "                    results.append(row['Tweet'])\n",
    "                    break\n",
    "    return results\n",
    "\n",
    "### ROUGE Metric Evaluation Model\n",
    "\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "# refs, datas\n",
    "refs = [gsHillary_favor,gsHillary_against,gsClimatechange_favor,gsAbortion_favor,gsAbortion_against,gsAtheism_favor,gsAtheism_against,gsFeministmovement_favor,\n",
    "       gsFeministmovement_against]\n",
    "\n",
    "datas = [H_fv,H_ag,Cl_fv,Ab_fv,Ab_ag,Ath_fv,Ath_ag,\n",
    "        Fm_fv,Fm_ag,]\n",
    "\n",
    "\n",
    "for data in datas:\n",
    "    sentence_vec(data)\n",
    "# datas = [model_hil_favor,model_hil_against,model_cc_favor,model_ab_favor,model_ab_against,model_ath_favor,model_ath_against,model_fm_favor,model_fm_against]\n",
    "\n",
    "names = ['Hillary_favor','Hillary_against','Climatechange_favor','Abortion_favor','Abortion_against','Atheism_favor','Atheism_against',\n",
    "        'Feministmovement_favor','Feministmovement_against']\n",
    "\n",
    "# sum_basic = orig_sumBasic(Fm_fv,len(gsFeministmovement_favor))\n",
    "\n",
    "# sum_basic\n",
    "\n",
    "# data, k = clustering_sen2v(Fm_fv)\n",
    "# bert_summarycos, eul = cluster_dis_summary(data,k,len(gsFeministmovement_favor))\n",
    "\n",
    "# bert_summarycos\n",
    "\n",
    "# data , k = clustering_w2v(Fm_fv,model_glove,200)\n",
    "# glove_summarycos, euc = cluster_dis_summary(data,k,len(gsFeministmovement_favor))\n",
    "\n",
    "# glove_summarycos\n",
    "\n",
    "# data , k = clustering_w2v(Fm_fv,model_w2v,300)\n",
    "# w2v_summarycos, euc = cluster_dis_summary(data,k,len(gsFeministmovement_favor))\n",
    "\n",
    "# w2v_summarycos\n",
    "\n",
    "# tr_1 = textrank(Fm_fv,gsFeministmovement_favor)\n",
    "\n",
    "# tr_1\n",
    "\n",
    "## Evaluation of summaries \n",
    "\n",
    "models = ['BERT','word2vec','glove']\n",
    "\n",
    "# evaluation for distance summary\n",
    "def evaluation(models):\n",
    "    list_of_scores = { 'Hillary_favor' :  {\"SB\" : None, \"textrank\" : None ,\"model_w2v\": None , \"model_glove\" : None, \"BERT\": None} ,\n",
    "                  'Hillary_against' :  { \"SB\" : None, \"textrank\" : None,\"model_w2v\" : None ,\"model_glove\" : None, \"BERT\": None}  ,\n",
    "                  'Climatechange_favor' :  {\"SB\": None,\"textrank\" : None, \"model_w2v\": None ,\"model_glove\" : None, \"BERT\": None} ,\n",
    "                  'Abortion_favor' : { \"SB\": None, \"textrank\" : None, \"model_w2v\": None ,\"model_glove\" : None, \"BERT\": None} ,\n",
    "                  'Abortion_against' :  {\"SB\": None, \"textrank\" : None, \"model_w2v\": None,\"model_glove\" : None, \"BERT\": None} ,\n",
    "                  'Atheism_favor' : { \"SB\": None,  \"textrank\" : None,\"model_w2v\": None,\"model_glove\" : None, \"BERT\": None } ,\n",
    "                  'Atheism_against':  { \"SB\" : None,\"textrank\" : None, \"model_w2v\": None ,\"model_glove\" : None, \"BERT\": None} ,\n",
    "                  'Feministmovement_favor' : {\"SB\": None,\"textrank\" : None,  \"model_w2v\": None,\"model_glove\" : None, \"BERT\": None } ,\n",
    "                  'Feministmovement_against' :  {\"SB\": None,\"textrank\" : None, \"model_w2v\" : None,\"model_glove\" : None, \"BERT\": None }  \n",
    "                 } \n",
    "        \n",
    "    for i in range(len(datas)):\n",
    "        if len(datas[i]) >= 65:\n",
    "            for model in models:\n",
    "                if model == 'word2vec': data, k= clustering_w2v(datas[i],model_w2v,300)\n",
    "                if model == 'BERT': data,k = clustering_sen2v(datas[i])\n",
    "                if model == 'glove': data,k = clustering_w2v(datas[i],model_glove,200)\n",
    "                cos_sum, eucl_sum = cluster_dis_summary(data,k,num_tweets=len(refs[i]))\n",
    "                sum_basic = orig_sumBasic(datas[i],len(refs[i]))\n",
    "                TR = textrank(datas[i],refs[i])\n",
    "                #sumbasic scores\n",
    "                scores_SB = rouge.get_scores(\" \".join(sum_basic),\" \".join(refs[i]['Tweet']))\n",
    "                scores_TR = rouge.get_scores(\" \".join(TR),\" \".join(refs[i]['Tweet']))\n",
    "                #model scores\n",
    "                scores_model = rouge.get_scores(\" \".join(cos_sum), \" \".join(refs[i]['Tweet']))\n",
    "                #add to our dictionary of scores\n",
    "                list_of_scores[names[i]][\"SB\"] = scores_SB\n",
    "                list_of_scores[names[i]][\"textrank\"] = scores_TR\n",
    "                if model == 'word2vec':  \n",
    "                    list_of_scores[names[i]][\"model_w2v\"] = scores_model \n",
    "                elif model == 'glove':\n",
    "                    list_of_scores[names[i]][\"model_glove\"] = scores_model\n",
    "                elif model == 'BERT':\n",
    "                    list_of_scores[names[i]][\"BERT\"] = scores_model\n",
    "                print('------------------Generated results for :', names[i])\n",
    "    return list_of_scores\n",
    "       \n",
    "        \n",
    "\n",
    "# evaluation for feature ranking summary\n",
    "def evaluation_2(model):\n",
    "    list_of_scores = { 'Hillary_favor' :  {\"SB\" : None, \"model_w2v\": None } ,\n",
    "                  'Hillary_against' :  { \"SB\" : None, \"model_w2v\" : None }  ,\n",
    "                  'Climatechange_favor' :  {\"SB\": None, \"model_w2v\": None } ,\n",
    "                  'Abortion_favor' : { \"SB\": None, \"model_w2v\": None } ,\n",
    "                  'Abortion_against' :  {\"SB\": None, \"model_w2v\": None } ,\n",
    "                  'Atheism_favor' : { \"SB\": None, \"model_w2v\": None } ,\n",
    "                  'Atheism_against':  { \"SB\" : None, \"model_w2v\": None } ,\n",
    "                  'Feministmovement_favor' : {\"SB\": None, \"model_w2v\": None } ,\n",
    "                  'Feministmovement_against' :  {\"SB\": None, \"model_w2v\" : None }  \n",
    "                 } \n",
    "        \n",
    "    for i in range(len(datas)):\n",
    "        if model == 'word2vec': data, k= clustering_w2v(datas[i],model_w2v)\n",
    "        if model == 'BERT': data,k = clustering_sen2v(datas[i])\n",
    "        summary = cluster_feature_summary(data,k,num_tweets=len(refs[i]))\n",
    "        sum_basic = orig_sumBasic(datas[i],len(refs[i]))\n",
    "        #sumbasic scores\n",
    "        scores_SB = rouge.get_scores(\" \".join(sum_basic),\" \".join(refs[i]['Tweet']))\n",
    "        #model scores\n",
    "        scores_model = rouge.get_scores(\" \".join(summary), \" \".join(refs[i]['Tweet']))\n",
    "        # add to our dictionary of scores\n",
    "        list_of_scores[names[i]][\"SB\"] = scores_SB\n",
    "        list_of_scores[names[i]][\"model_w2v\"] = scores_model\n",
    "        print('------ Generated results for :', names[i])\n",
    "#         print(list_of_scores)\n",
    "    return list_of_scores\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def SB_avg_all(scores):\n",
    "    f1_1 = 0\n",
    "    f1_2 = 0\n",
    "    f1_l = 0\n",
    "    for i in names:\n",
    "        f1_1+= scores[i]['SB'][0]['rouge-1']['f']\n",
    "        f1_2+= scores[i]['SB'][0]['rouge-2']['f']\n",
    "        f1_l+= scores[i]['SB'][0]['rouge-l']['f']\n",
    "    return float(f1_1)/9 , float(f1_2) / 9 , float(f1_l) / 9\n",
    "\n",
    "\n",
    "def TR_avg_all(scores):\n",
    "    f1_1 = 0\n",
    "    f1_2 = 0\n",
    "    f1_l = 0\n",
    "    for i in names:\n",
    "        f1_1+= scores[i]['textrank'][0]['rouge-1']['f']\n",
    "        f1_2+= scores[i]['textrank'][0]['rouge-2']['f']\n",
    "        f1_l+= scores[i]['textrank'][0]['rouge-l']['f']\n",
    "    return float(f1_1)/9 , float(f1_2) / 9 , float(f1_l) / 9\n",
    "\n",
    "def w2v_avg_all(scores):\n",
    "    f1_1 = 0\n",
    "    f1_2 = 0\n",
    "    f1_l = 0\n",
    "    for i in names:\n",
    "        f1_1+= scores[i]['model_w2v'][0]['rouge-1']['f']\n",
    "        f1_2+= scores[i]['model_w2v'][0]['rouge-2']['f']\n",
    "        f1_l+= scores[i]['model_w2v'][0]['rouge-l']['f']\n",
    "    return float(f1_1)/9 , float(f1_2) / 9 , float(f1_l) / 9\n",
    "\n",
    "\n",
    "\n",
    "def glove_avg_all(scores):\n",
    "    f1_1 = 0\n",
    "    f1_2 = 0\n",
    "    f1_l = 0\n",
    "    for i in names:\n",
    "        f1_1+= scores[i]['model_glove'][0]['rouge-1']['f']\n",
    "        f1_2+= scores[i]['model_glove'][0]['rouge-2']['f']\n",
    "        f1_l+= scores[i]['model_glove'][0]['rouge-l']['f']\n",
    "    return float(f1_1)/9 , float(f1_2) / 9 , float(f1_l) / 9\n",
    "\n",
    "\n",
    "def BERT_avg_all(scores):\n",
    "    f1_1 = 0\n",
    "    f1_2 = 0\n",
    "    f1_l = 0\n",
    "    for i in names:\n",
    "        f1_1+= scores[i]['BERT'][0]['rouge-1']['f']\n",
    "        f1_2+= scores[i]['BERT'][0]['rouge-2']['f']\n",
    "        f1_l+= scores[i]['BERT'][0]['rouge-l']['f']\n",
    "    return float(f1_1)/9 , float(f1_2) / 9 , float(f1_l) / 9\n",
    "\n",
    "\n",
    "def scoring(scores):\n",
    "    SB = SB_avg_all(scores)\n",
    "    TR = TR_avg_all(scores)\n",
    "    glove = glove_avg_all(scores)\n",
    "    w2v = w2v_avg_all(scores)\n",
    "    BERT = BERT_avg_all(scores)\n",
    "    return SB,TR,glove,w2v,BERT\n",
    "\n",
    "\n",
    "scores = evaluation(models)\n",
    "SB,TR,glove,w2v,BERT = scoring(scores)\n",
    "\n",
    "print(SB)\n",
    "print(TR)\n",
    "print(glove)\n",
    "print(BERT)\n",
    "print(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31725ed5-aeb5-4e7c-9a05-4a75cf65cc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Climatechange['Stance'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fb0c2-4ffc-4abc-9c9d-d16efa6c1f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c91c87-5f52-4dfb-b340-e1f899e817bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flair_nlp",
   "language": "python",
   "name": "flair_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
